# -*- coding: utf-8 -*-
"""major project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13S-QbZTtQ4Wx-ZXQxS04Kj81PgU5EEbN
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('amazon_alexa_data.csv')

data.shape

data.head()

data.tail()

data.isnull().sum()

data.describe()

data['length'] = data['verified_reviews'].apply(len)

data.shape

data.head()

data.groupby('rating').describe

data.groupby('feedback').describe

data['rating'].value_counts().plot.bar(color = 'Red')
plt.title('Visualizing the ratings dist.')
plt.xlabel('ratings')
plt.ylabel('count')
plt.show()

data['rating'].value_counts()

labels = '5', '4', '3', '2', '1'
sizes = [2286, 455, 161, 152, 96]
colors = ['yellow', 'magenta', 'pink', 'cyan', 'red']
explode = [0.001, 0.001, 0.001, 0.001, 0.001]

plt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True)
plt.title(' pie chart representing ratings occuposition')
plt.show()

data['variation'].value_counts().plot.bar(color = 'blue', figsize = (11, 7))
plt.title('Visualizing the variations dist.')
plt.xlabel('variations')
plt.ylabel('count')
plt.show()

data['feedback'].value_counts().plot.bar(color = 'orange', figsize = (5, 4))
plt.title('Visualizing the feedbacks dist.')
plt.xlabel('feedbacks')
plt.ylabel('count')
plt.show()

data['length'].value_counts().plot.hist(color = 'pink', figsize = (12, 5), bins = 50)
plt.title('Visualizing the length dist.')
plt.xlabel('lengths')
plt.ylabel('count')
plt.show()

data.length.describe()

data[data['length'] == 1]['verified_reviews'].iloc[0]

data[data['length'] == 21]['verified_reviews'].iloc[0]

data[data['length'] == 50]['verified_reviews'].iloc[0]

data.date.describe()

data['date'].value_counts()

data.groupby('variation').mean()[['rating']].plot.bar(color = 'brown', figsize=(11, 6))
plt.title("Variation wise Mean Ratings")
plt.xlabel('variatiions')
plt.ylabel('ratings')
plt.show()

data.groupby('feedback').mean()[['rating']].plot.bar(color = 'crimson', figsize=(5, 4))
plt.title("feedback wise Mean Ratings")
plt.xlabel('feedbacks')
plt.ylabel('ratings')
plt.show()

data.groupby('length').mean()[['rating']].plot.hist(color = 'gray', figsize=(7, 6), bins = 20)
plt.title("length wise Mean Ratings")
plt.xlabel('length')
plt.ylabel('ratings')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words = 'english')
words = cv.fit_transform(data.verified_reviews)
sum_words = words.sum(axis=0)

words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)
frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])
frequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'lightgreen')
plt.title("Most Frequently Occuring Words - Top 20")

from wordcloud import WordCloud

wordcloud = WordCloud(background_color = 'lightcyan', width = 1200, height = 700).generate_from_frequencies(dict(words_freq))

plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.title("WordCloud - Vocabulary from Reviews", fontsize = 22)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus = []

for i in range(0, 3150):
  review = re.sub('[^a-zA-Z]', ' ', data['verified_reviews'][i])
  review = review.lower()
  review = review.split()
  ps = PorterStemmer()
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  review = ' '.join(review)
  corpus.append(review)

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features = 2500)

x = cv.fit_transform(corpus).toarray()

y = data.iloc[:, 4].values

print(x.shape)

print(y.shape)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

from sklearn.linear_model import LogisticRegression

m1 = LogisticRegression()
m1.fit(x_train,y_train)

ypred_m1 = m1.predict(x_test)
print(ypred_m1)

print('Training score',m1.score(x_train,y_train))
print('Testing score',m1.score(x_test,y_test))

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

cm = confusion_matrix(y_test,ypred_m1)
print(cm)
print(classification_report(y_test,ypred_m1))

print('Testing score',m1.score(x_test,y_test))
print('Accuracy score',accuracy_score(y_test,ypred_m1))

from sklearn.naive_bayes import MultinomialNB, GaussianNB
print("Naive Bayes")
m2 = MultinomialNB()
m2.fit(x_train,y_train)
print("Training score",m2.score(x_train,y_train))
print("Testing sscore",m2.score(x_test,y_test))

ypred_m2 = m2.predict(x_test)
print(ypred_m2)

cm = confusion_matrix(y_test,ypred_m2)
print(cm)
print(classification_report(y_test,ypred_m2))

from sklearn.neighbors import KNeighborsClassifier

m3 = KNeighborsClassifier(n_neighbors=19)
m3.fit(x_train,y_train)

print('Training score',m3.score(x_train,y_train))
print('Testing score',m3.score(x_test,y_test))

ypred_m3 = m3.predict(x_test)
print(ypred_m3)

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

cm = confusion_matrix(y_test,ypred_m3)
print(cm)
print(classification_report(y_test,ypred_m3))

print(accuracy_score(y_test,ypred_m3))
print('Testing score',m3.score(x_test,y_test))



